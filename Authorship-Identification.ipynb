{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09915501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/arodi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/arodi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/arodi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from string import digits\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e740aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train labels 50\n",
      "Train labels ['RobinSidel', 'LynnleyBrowning', 'KouroshKarimkhany', 'MichaelConnor', 'JoeOrtiz', 'EricAuchard', 'AaronPressman', 'SimonCowell', \"LynneO'Donnell\", 'EdnaFernandes', 'KevinMorrison', 'SamuelPerry', 'PatriciaCommins', 'JohnMastrini', 'JanLopatka', 'KevinDrawbaugh', 'KarlPenhaul', 'MartinWolk', 'ScottHillis', 'DavidLawder', 'FumikoFujisaki', 'MarcelMichelson', 'NickLouth', 'DarrenSchuettler', 'WilliamKazer', 'TanEeLyn', 'PierreTran', 'HeatherScoffield', 'MureDickie', 'RogerFillion', 'JimGilchrist', 'BradDorfman', 'AlanCrosby', 'JonathanBirt', 'BenjaminKangLim', 'TheresePoletti', 'KeithWeir', 'JoWinterbottom', 'MarkBendeich', 'JaneMacartney', 'MatthewBunce', 'ToddNissen', 'PeterHumphrey', 'TimFarrand', 'SarahDavison', 'GrahamEarnshaw', 'BernardHickey', 'KirstinRidley', 'AlexanderSmith', 'LydiaZajc']\n",
      "--------------------------------------------------\n",
      "Number of test labels: 50\n",
      "Test labels ['RobinSidel', 'LynnleyBrowning', 'KouroshKarimkhany', 'MichaelConnor', 'JoeOrtiz', 'EricAuchard', 'AaronPressman', 'SimonCowell', \"LynneO'Donnell\", 'EdnaFernandes', 'KevinMorrison', 'SamuelPerry', 'PatriciaCommins', 'JohnMastrini', 'JanLopatka', 'KevinDrawbaugh', 'KarlPenhaul', 'MartinWolk', 'ScottHillis', 'DavidLawder', 'FumikoFujisaki', 'MarcelMichelson', 'NickLouth', 'DarrenSchuettler', 'WilliamKazer', 'TanEeLyn', 'PierreTran', 'HeatherScoffield', 'MureDickie', 'RogerFillion', 'JimGilchrist', 'BradDorfman', 'AlanCrosby', 'JonathanBirt', 'BenjaminKangLim', 'TheresePoletti', 'KeithWeir', 'JoWinterbottom', 'MarkBendeich', 'JaneMacartney', 'MatthewBunce', 'ToddNissen', 'PeterHumphrey', 'TimFarrand', 'SarahDavison', 'GrahamEarnshaw', 'BernardHickey', 'KirstinRidley', 'AlexanderSmith', 'LydiaZajc']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#separate training data and labels\n",
    "#print(os.path)\n",
    "train_data_path='/Users/arodi/Desktop/Fall-2021/CMPE-255/project/CMPE-255_S3-Group10/C50/C50train/'\n",
    "#change the data pathname while running based on location\n",
    "author_list=os.listdir(train_data_path)#lists all the authors in path 'train_data_path'\n",
    "train_labels=[]\n",
    "author_text_tuple=[]\n",
    "train_set=[]\n",
    "authindex=0\n",
    "#print('l',l)\n",
    "for p in author_list:\n",
    "    #print(p)\n",
    "    if (not p.startswith('.')):\n",
    "        file_list=os.listdir(train_data_path+p)#lists all the files in the path \"train_data_path+p\"\n",
    "        #file_list.remove('.ipynb_checkpoints') # remove .ipynb_checkpoints if data is present  in jupyter notebook\n",
    "        #file_list.remove('.DS_store') # remove .DS_store files if data is prsent in local drive\n",
    "        path2_each_author=train_data_path+p#path for every author folder\n",
    "        #print('file_list',file_list) #print all text files names for each author\n",
    "        \n",
    "        for author in author_list:\n",
    "            if (not author.startswith('.')):  # this condition is to keep out all hidden files\n",
    "                if author not in train_labels:\n",
    "                    train_labels.append(author)\n",
    "        writings = ''\n",
    "        for file in range(len(file_list)):\n",
    "            #print(path2_each_author+'/'+file)\n",
    "            with open (path2_each_author+'/'+file_list[file],'r') as fp:\n",
    "                writings+=fp.read()\n",
    "                #author_text_tuple.append((author_list[authindex],file_list[file],(fp.read().splitlines())))\n",
    "        train_set.append(writings)\n",
    "        #authindex+=1\n",
    "print(\"Number of train labels\",len(train_labels))\n",
    "print(\"Train labels\",train_labels)#list of authors which are class labels\n",
    "print(\"-\"*50)\n",
    "#print(author_text_tuple)# list of tuples with filename,content\n",
    "#print(train_set)# print combined text from each author\n",
    "\n",
    "        \n",
    "#separate test data and labels\n",
    "#print(os.path)\n",
    "test_data_path='/Users/arodi/Desktop/Fall-2021/CMPE-255/project/CMPE-255_S3-Group10/C50/C50test/'\n",
    "author_list=os.listdir(test_data_path)#lists all the authors in path 'train_data_path'\n",
    "test_labels=[]\n",
    "author_text_tuple=[]\n",
    "test_set=[]\n",
    "authindex=0\n",
    "#print('l',l)\n",
    "for p in author_list:\n",
    "    #print(p)\n",
    "    if (not p.startswith('.')):\n",
    "        file_list=os.listdir(test_data_path+p)#lists all the files in the path \"test_data_path+p\"\n",
    "        #file_list.remove('.ipynb_checkpoints') # remove .ipynb_checkpoints if data is present  in jupyter notebook\n",
    "        #file_list.remove('.DS_store') # remove .DS_store files if data is prsent in local drive\n",
    "        path2_each_author=test_data_path+p#path for every author folder\n",
    "        #print('file_list',file_list)\n",
    "        \n",
    "        for author in author_list:\n",
    "            if (not author.startswith('.')):  # this condition is to keep out all hidden files\n",
    "                if author not in test_labels:\n",
    "                    test_labels.append(author)\n",
    "        writings = ''\n",
    "        for file in range(len(file_list)):\n",
    "            #print(path2_each_author+'/'+file)\n",
    "            with open (path2_each_author+'/'+file_list[file],'r') as fp:\n",
    "                writings+=fp.read()\n",
    "                #author_text_tuple.append((author_list[authindex],file_list[file],(fp.read().splitlines())))\n",
    "        test_set.append(writings)\n",
    "        #authindex+=1\n",
    "print(\"Number of test labels:\",len(train_labels))\n",
    "print(\"Test labels\",train_labels)#list of authors which are class labels\n",
    "print(\"-\"*50)\n",
    "#print(author_text_tuple)# list of tuples with filename,content\n",
    "#print(test_set)# print combined text from each author \n",
    "\n",
    "# some data preprocessing todo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2086504",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6})|\\S*@\\S*\\s?;')\n",
    "\n",
    "\n",
    "def preprocess(reviews):\n",
    "    processed_text = []\n",
    "    for review in reviews:\n",
    "        # remove html tags  and email\n",
    "        removed = re.sub(remover, '', review)\n",
    "        to_remove = str.maketrans('', '', digits)\n",
    "        number_removed = removed.translate(to_remove)\n",
    "        #remove url\n",
    "        url_removed = re.sub('r^https?:\\/\\/.*[\\r\\n]*', '', number_removed, flags=re.MULTILINE)\n",
    "        #punctuation removed\n",
    "        punctuation_removed=url_removed.translate(str.maketrans('', '', string.punctuation))\n",
    "        # lowercase\n",
    "        lower = punctuation_removed.lower()\n",
    "        # lemmatize\n",
    "        clean_text = \"\"\n",
    "        stop_removed = \"\"\n",
    "        lemmatized_text = \"\"\n",
    "        # remove stop words using dictionary dictionary is faster than sets\n",
    "        words_to_remove = stopwords.words('english')\n",
    "        stopword_list = Counter(words_to_remove)\n",
    "        tokens = word_tokenize(lower)\n",
    "        tokens = [t.strip() for t in tokens]\n",
    "        words_to_keep = [t for t in tokens if t not in stopword_list]\n",
    "        stop_removed = \" \".join(words_to_keep)\n",
    "        # lemmatization is done based on context\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokenization = nltk.word_tokenize(stop_removed)\n",
    "        for w in tokenization:\n",
    "            clean_text += (lemmatizer.lemmatize(w))+\" \"\n",
    "        processed_text.append(clean_text.strip())\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42678f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# clean train and test data\n",
    "processed_train = preprocess(train_set)\n",
    "print(len(processed_train))\n",
    "processed_test = preprocess(test_set)\n",
    "print(len(processed_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ed843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 410102)\n",
      "(50, 410102)\n"
     ]
    }
   ],
   "source": [
    "# vectorize the writings\n",
    "count_vectorizer = CountVectorizer(strip_accents='ascii',ngram_range=(1, 2))\n",
    "cvmattrain = count_vectorizer.fit_transform(processed_train)\n",
    "cvmattest = count_vectorizer.transform(processed_test)\n",
    "\n",
    "\n",
    "#normalize data\n",
    "tfidf_trasnformer =  TfidfTransformer()\n",
    "tfidftrain = tfidf_trasnformer.fit_transform(cvmattrain)\n",
    "tfidftest = tfidf_trasnformer.fit_transform(cvmattest)\n",
    "\n",
    "#convert to csr matrix\n",
    "\n",
    "compressed_train = csr_matrix(tfidftrain)\n",
    "compressed_test = csr_matrix(tfidftest)\n",
    "\n",
    "print(compressed_train.shape)\n",
    "print(compressed_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cfbcef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Linear SVC-----------\n",
      "Accuracy: 0.88\n",
      "F1 score: 0.88\n",
      "Recall score: 0.88\n",
      "Precision score: 0.88\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       1.00      1.00      1.00         1\n",
      "       AlanCrosby       1.00      1.00      1.00         1\n",
      "   AlexanderSmith       1.00      0.00      0.00         1\n",
      "  BenjaminKangLim       1.00      1.00      1.00         1\n",
      "    BernardHickey       1.00      1.00      1.00         1\n",
      "      BradDorfman       1.00      1.00      1.00         1\n",
      " DarrenSchuettler       1.00      0.00      0.00         1\n",
      "      DavidLawder       1.00      0.00      0.00         1\n",
      "    EdnaFernandes       1.00      1.00      1.00         1\n",
      "      EricAuchard       1.00      1.00      1.00         1\n",
      "   FumikoFujisaki       1.00      1.00      1.00         1\n",
      "   GrahamEarnshaw       1.00      1.00      1.00         1\n",
      " HeatherScoffield       0.50      1.00      0.67         1\n",
      "       JanLopatka       1.00      0.00      0.00         1\n",
      "    JaneMacartney       1.00      0.00      0.00         1\n",
      "     JimGilchrist       1.00      1.00      1.00         1\n",
      "   JoWinterbottom       1.00      1.00      1.00         1\n",
      "         JoeOrtiz       0.50      1.00      0.67         1\n",
      "     JohnMastrini       0.50      1.00      0.67         1\n",
      "     JonathanBirt       1.00      1.00      1.00         1\n",
      "      KarlPenhaul       1.00      1.00      1.00         1\n",
      "        KeithWeir       1.00      1.00      1.00         1\n",
      "   KevinDrawbaugh       1.00      1.00      1.00         1\n",
      "    KevinMorrison       1.00      1.00      1.00         1\n",
      "    KirstinRidley       1.00      1.00      1.00         1\n",
      "KouroshKarimkhany       1.00      1.00      1.00         1\n",
      "        LydiaZajc       1.00      1.00      1.00         1\n",
      "   LynneO'Donnell       1.00      1.00      1.00         1\n",
      "  LynnleyBrowning       1.00      1.00      1.00         1\n",
      "  MarcelMichelson       1.00      1.00      1.00         1\n",
      "     MarkBendeich       1.00      1.00      1.00         1\n",
      "       MartinWolk       1.00      1.00      1.00         1\n",
      "     MatthewBunce       1.00      1.00      1.00         1\n",
      "    MichaelConnor       1.00      1.00      1.00         1\n",
      "       MureDickie       0.50      1.00      0.67         1\n",
      "        NickLouth       1.00      1.00      1.00         1\n",
      "  PatriciaCommins       1.00      1.00      1.00         1\n",
      "    PeterHumphrey       1.00      1.00      1.00         1\n",
      "       PierreTran       1.00      1.00      1.00         1\n",
      "       RobinSidel       1.00      1.00      1.00         1\n",
      "     RogerFillion       1.00      1.00      1.00         1\n",
      "      SamuelPerry       1.00      1.00      1.00         1\n",
      "     SarahDavison       1.00      1.00      1.00         1\n",
      "      ScottHillis       1.00      0.00      0.00         1\n",
      "      SimonCowell       1.00      1.00      1.00         1\n",
      "         TanEeLyn       1.00      1.00      1.00         1\n",
      "   TheresePoletti       1.00      1.00      1.00         1\n",
      "       TimFarrand       1.00      1.00      1.00         1\n",
      "       ToddNissen       0.50      1.00      0.67         1\n",
      "     WilliamKazer       0.50      1.00      0.67         1\n",
      "\n",
      "         accuracy                           0.88        50\n",
      "        macro avg       0.94      0.88      0.84        50\n",
      "     weighted avg       0.94      0.88      0.84        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----------Linear SVC-----------\")#0.88\n",
    "#train the classifier, using linear svc\n",
    "lsvcclf  = LinearSVC(C=10,max_iter=4000)\n",
    "lsvcclf.fit(compressed_train, train_labels)\n",
    "predicted_classes = lsvcclf.predict(compressed_test) # actual prediction of labels for test data\n",
    "#print(predicted_classes)\n",
    "accuracy_lsvc = accuracy_score(test_labels,predicted_classes)\n",
    "f1_lsvc = f1_score(test_labels,predicted_classes, average='micro')\n",
    "precision_lsvc = precision_score(test_labels,predicted_classes, average='micro')\n",
    "recall_lsvc = recall_score(test_labels,predicted_classes, average='micro')\n",
    "print(\"Accuracy:\",accuracy_lsvc)\n",
    "print(\"F1 score:\",f1_lsvc)\n",
    "print(\"Recall score:\",recall_lsvc)\n",
    "print(\"Precision score:\",precision_lsvc)\n",
    "# prepare a classification report\n",
    "print(classification_report(test_labels, predicted_classes,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c8d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Naive Bayes-----------\n",
      "Accuracy: 0.88\n",
      "F1 score: 0.88\n",
      "Recall score: 0.88\n",
      "Precision score: 0.88\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       1.00      1.00      1.00         1\n",
      "       AlanCrosby       1.00      1.00      1.00         1\n",
      "   AlexanderSmith       1.00      0.00      0.00         1\n",
      "  BenjaminKangLim       1.00      0.00      0.00         1\n",
      "    BernardHickey       1.00      1.00      1.00         1\n",
      "      BradDorfman       1.00      1.00      1.00         1\n",
      " DarrenSchuettler       1.00      0.00      0.00         1\n",
      "      DavidLawder       1.00      0.00      0.00         1\n",
      "    EdnaFernandes       1.00      1.00      1.00         1\n",
      "      EricAuchard       1.00      1.00      1.00         1\n",
      "   FumikoFujisaki       1.00      1.00      1.00         1\n",
      "   GrahamEarnshaw       1.00      1.00      1.00         1\n",
      " HeatherScoffield       0.50      1.00      0.67         1\n",
      "       JanLopatka       1.00      1.00      1.00         1\n",
      "    JaneMacartney       1.00      0.00      0.00         1\n",
      "     JimGilchrist       1.00      1.00      1.00         1\n",
      "   JoWinterbottom       1.00      1.00      1.00         1\n",
      "         JoeOrtiz       0.50      1.00      0.67         1\n",
      "     JohnMastrini       1.00      1.00      1.00         1\n",
      "     JonathanBirt       1.00      1.00      1.00         1\n",
      "      KarlPenhaul       1.00      1.00      1.00         1\n",
      "        KeithWeir       1.00      1.00      1.00         1\n",
      "   KevinDrawbaugh       1.00      1.00      1.00         1\n",
      "    KevinMorrison       1.00      1.00      1.00         1\n",
      "    KirstinRidley       1.00      1.00      1.00         1\n",
      "KouroshKarimkhany       1.00      1.00      1.00         1\n",
      "        LydiaZajc       1.00      1.00      1.00         1\n",
      "   LynneO'Donnell       1.00      1.00      1.00         1\n",
      "  LynnleyBrowning       1.00      1.00      1.00         1\n",
      "  MarcelMichelson       1.00      1.00      1.00         1\n",
      "     MarkBendeich       1.00      1.00      1.00         1\n",
      "       MartinWolk       1.00      1.00      1.00         1\n",
      "     MatthewBunce       1.00      1.00      1.00         1\n",
      "    MichaelConnor       1.00      1.00      1.00         1\n",
      "       MureDickie       1.00      1.00      1.00         1\n",
      "        NickLouth       1.00      1.00      1.00         1\n",
      "  PatriciaCommins       1.00      1.00      1.00         1\n",
      "    PeterHumphrey       1.00      1.00      1.00         1\n",
      "       PierreTran       1.00      1.00      1.00         1\n",
      "       RobinSidel       1.00      1.00      1.00         1\n",
      "     RogerFillion       1.00      1.00      1.00         1\n",
      "      SamuelPerry       1.00      1.00      1.00         1\n",
      "     SarahDavison       1.00      0.00      0.00         1\n",
      "      ScottHillis       0.33      1.00      0.50         1\n",
      "      SimonCowell       1.00      1.00      1.00         1\n",
      "         TanEeLyn       0.50      1.00      0.67         1\n",
      "   TheresePoletti       1.00      1.00      1.00         1\n",
      "       TimFarrand       1.00      1.00      1.00         1\n",
      "       ToddNissen       0.50      1.00      0.67         1\n",
      "     WilliamKazer       1.00      1.00      1.00         1\n",
      "\n",
      "         accuracy                           0.88        50\n",
      "        macro avg       0.95      0.88      0.84        50\n",
      "     weighted avg       0.95      0.88      0.84        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----------Naive Bayes-----------\") #0.88\n",
    "#train the classifier, using mnb\n",
    "mnbclf  = MultinomialNB()\n",
    "mnbclf.fit(compressed_train, train_labels)\n",
    "predicted_classes_mnb = mnbclf.predict(compressed_test) # actual prediction of labels for test data\n",
    "#print(predicted_classes)\n",
    "accuracy_mnb = accuracy_score(test_labels,predicted_classes_mnb)\n",
    "f1_mnb = f1_score(test_labels,predicted_classes_mnb, average='micro')\n",
    "precision_mnb = precision_score(test_labels,predicted_classes_mnb, average='micro')\n",
    "recall_mnb = recall_score(test_labels,predicted_classes_mnb, average='micro')\n",
    "print(\"Accuracy:\",accuracy_mnb)\n",
    "print(\"F1 score:\",f1_mnb)\n",
    "print(\"Recall score:\",recall_mnb)\n",
    "print(\"Precision score:\",precision_mnb)\n",
    "# prepare a classification report\n",
    "print(classification_report(test_labels, predicted_classes_mnb,zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40eea1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Logistic Regression-----------\n",
      "Accuracy: 0.86\n",
      "F1 score: 0.8599999999999999\n",
      "Recall score: 0.86\n",
      "Precision score: 0.86\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       1.00      1.00      1.00         1\n",
      "       AlanCrosby       1.00      1.00      1.00         1\n",
      "   AlexanderSmith       1.00      0.00      0.00         1\n",
      "  BenjaminKangLim       1.00      0.00      0.00         1\n",
      "    BernardHickey       1.00      1.00      1.00         1\n",
      "      BradDorfman       1.00      1.00      1.00         1\n",
      " DarrenSchuettler       1.00      0.00      0.00         1\n",
      "      DavidLawder       1.00      0.00      0.00         1\n",
      "    EdnaFernandes       1.00      1.00      1.00         1\n",
      "      EricAuchard       1.00      1.00      1.00         1\n",
      "   FumikoFujisaki       1.00      1.00      1.00         1\n",
      "   GrahamEarnshaw       1.00      1.00      1.00         1\n",
      " HeatherScoffield       0.50      1.00      0.67         1\n",
      "       JanLopatka       1.00      0.00      0.00         1\n",
      "    JaneMacartney       1.00      0.00      0.00         1\n",
      "     JimGilchrist       1.00      1.00      1.00         1\n",
      "   JoWinterbottom       1.00      1.00      1.00         1\n",
      "         JoeOrtiz       0.50      1.00      0.67         1\n",
      "     JohnMastrini       0.50      1.00      0.67         1\n",
      "     JonathanBirt       1.00      1.00      1.00         1\n",
      "      KarlPenhaul       1.00      1.00      1.00         1\n",
      "        KeithWeir       1.00      1.00      1.00         1\n",
      "   KevinDrawbaugh       1.00      1.00      1.00         1\n",
      "    KevinMorrison       1.00      1.00      1.00         1\n",
      "    KirstinRidley       1.00      1.00      1.00         1\n",
      "KouroshKarimkhany       1.00      1.00      1.00         1\n",
      "        LydiaZajc       1.00      1.00      1.00         1\n",
      "   LynneO'Donnell       1.00      1.00      1.00         1\n",
      "  LynnleyBrowning       1.00      1.00      1.00         1\n",
      "  MarcelMichelson       1.00      1.00      1.00         1\n",
      "     MarkBendeich       1.00      1.00      1.00         1\n",
      "       MartinWolk       1.00      1.00      1.00         1\n",
      "     MatthewBunce       1.00      1.00      1.00         1\n",
      "    MichaelConnor       1.00      1.00      1.00         1\n",
      "       MureDickie       1.00      1.00      1.00         1\n",
      "        NickLouth       1.00      1.00      1.00         1\n",
      "  PatriciaCommins       1.00      1.00      1.00         1\n",
      "    PeterHumphrey       1.00      1.00      1.00         1\n",
      "       PierreTran       1.00      1.00      1.00         1\n",
      "       RobinSidel       1.00      1.00      1.00         1\n",
      "     RogerFillion       1.00      1.00      1.00         1\n",
      "      SamuelPerry       1.00      1.00      1.00         1\n",
      "     SarahDavison       1.00      0.00      0.00         1\n",
      "      ScottHillis       0.33      1.00      0.50         1\n",
      "      SimonCowell       1.00      1.00      1.00         1\n",
      "         TanEeLyn       0.50      1.00      0.67         1\n",
      "   TheresePoletti       1.00      1.00      1.00         1\n",
      "       TimFarrand       1.00      1.00      1.00         1\n",
      "       ToddNissen       0.50      1.00      0.67         1\n",
      "     WilliamKazer       1.00      1.00      1.00         1\n",
      "\n",
      "         accuracy                           0.86        50\n",
      "        macro avg       0.94      0.86      0.82        50\n",
      "     weighted avg       0.94      0.86      0.82        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----------Logistic Regression-----------\")#0.86\n",
    "#train the classifier, Logistic regression\n",
    "lrclf  = LogisticRegression(C=10)\n",
    "lrclf.fit(compressed_train, train_labels)\n",
    "predicted_classes_lr = lrclf.predict(compressed_test) # actual prediction of labels for test data\n",
    "#print(predicted_classes)\n",
    "accuracy_lr = accuracy_score(test_labels,predicted_classes_lr)\n",
    "f1_lr = f1_score(test_labels,predicted_classes_lr, average='micro')\n",
    "precision_lr = precision_score(test_labels,predicted_classes_lr, average='micro')\n",
    "recall_lr = recall_score(test_labels,predicted_classes_lr, average='micro')\n",
    "print(\"Accuracy:\",accuracy_lr)\n",
    "print(\"F1 score:\",f1_lr)\n",
    "print(\"Recall score:\",recall_lr)\n",
    "print(\"Precision score:\",precision_lr)\n",
    "# prepare a classification report\n",
    "print(classification_report(test_labels, predicted_classes_lr,zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d15e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
