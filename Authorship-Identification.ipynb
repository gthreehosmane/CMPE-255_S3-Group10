{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639dc266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/arodi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/arodi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/arodi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from string import digits\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa1001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train labels 50\n",
      "Train labels ['RobinSidel', 'LynnleyBrowning', 'KouroshKarimkhany', 'MichaelConnor', 'JoeOrtiz', 'EricAuchard', 'AaronPressman', 'SimonCowell', \"LynneO'Donnell\", 'EdnaFernandes', 'KevinMorrison', 'SamuelPerry', 'PatriciaCommins', 'JohnMastrini', 'JanLopatka', 'KevinDrawbaugh', 'KarlPenhaul', 'MartinWolk', 'ScottHillis', 'DavidLawder', 'FumikoFujisaki', 'MarcelMichelson', 'NickLouth', 'DarrenSchuettler', 'WilliamKazer', 'TanEeLyn', 'PierreTran', 'HeatherScoffield', 'MureDickie', 'RogerFillion', 'JimGilchrist', 'BradDorfman', 'AlanCrosby', 'JonathanBirt', 'BenjaminKangLim', 'TheresePoletti', 'KeithWeir', 'JoWinterbottom', 'MarkBendeich', 'JaneMacartney', 'MatthewBunce', 'ToddNissen', 'PeterHumphrey', 'TimFarrand', 'SarahDavison', 'GrahamEarnshaw', 'BernardHickey', 'KirstinRidley', 'AlexanderSmith', 'LydiaZajc']\n",
      "--------------------------------------------------\n",
      "Number of test labels: 50\n",
      "Test labels ['RobinSidel', 'LynnleyBrowning', 'KouroshKarimkhany', 'MichaelConnor', 'JoeOrtiz', 'EricAuchard', 'AaronPressman', 'SimonCowell', \"LynneO'Donnell\", 'EdnaFernandes', 'KevinMorrison', 'SamuelPerry', 'PatriciaCommins', 'JohnMastrini', 'JanLopatka', 'KevinDrawbaugh', 'KarlPenhaul', 'MartinWolk', 'ScottHillis', 'DavidLawder', 'FumikoFujisaki', 'MarcelMichelson', 'NickLouth', 'DarrenSchuettler', 'WilliamKazer', 'TanEeLyn', 'PierreTran', 'HeatherScoffield', 'MureDickie', 'RogerFillion', 'JimGilchrist', 'BradDorfman', 'AlanCrosby', 'JonathanBirt', 'BenjaminKangLim', 'TheresePoletti', 'KeithWeir', 'JoWinterbottom', 'MarkBendeich', 'JaneMacartney', 'MatthewBunce', 'ToddNissen', 'PeterHumphrey', 'TimFarrand', 'SarahDavison', 'GrahamEarnshaw', 'BernardHickey', 'KirstinRidley', 'AlexanderSmith', 'LydiaZajc']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#separate training data and labels\n",
    "#print(os.path)\n",
    "train_data_path='/Users/arodi/Desktop/Fall-2021/CMPE-255/project/CMPE-255_S3-Group10/C50/C50train/'\n",
    "#change the data pathname while running based on location\n",
    "author_list=os.listdir(train_data_path)#lists all the authors in path 'train_data_path'\n",
    "train_labels=[]\n",
    "author_text_tuple=[]\n",
    "train_set=[]\n",
    "authindex=0\n",
    "#print('l',l)\n",
    "for p in author_list:\n",
    "    #print(p)\n",
    "    if (not p.startswith('.')):\n",
    "        file_list=os.listdir(train_data_path+p)#lists all the files in the path \"train_data_path+p\"\n",
    "        #file_list.remove('.ipynb_checkpoints') # remove .ipynb_checkpoints if data is present  in jupyter notebook\n",
    "        #file_list.remove('.DS_store') # remove .DS_store files if data is prsent in local drive\n",
    "        path2_each_author=train_data_path+p#path for every author folder\n",
    "        #print('file_list',file_list) #print all text files names for each author\n",
    "        \n",
    "        for author in author_list:\n",
    "            if (not author.startswith('.')):  # this condition is to keep out all hidden files\n",
    "                if author not in train_labels:\n",
    "                    train_labels.append(author)\n",
    "        writings = ''\n",
    "        for file in range(len(file_list)):\n",
    "            #print(path2_each_author+'/'+file)\n",
    "            with open (path2_each_author+'/'+file_list[file],'r') as fp:\n",
    "                writings+=fp.read()\n",
    "                #author_text_tuple.append((author_list[authindex],file_list[file],(fp.read().splitlines())))\n",
    "        train_set.append(writings)\n",
    "        #authindex+=1\n",
    "print(\"Number of train labels\",len(train_labels))\n",
    "print(\"Train labels\",train_labels)#list of authors which are class labels\n",
    "print(\"-\"*50)\n",
    "#print(author_text_tuple)# list of tuples with filename,content\n",
    "#print(train_set)# print combined text from each author\n",
    "\n",
    "        \n",
    "#separate test data and labels\n",
    "#print(os.path)\n",
    "test_data_path='/Users/arodi/Desktop/Fall-2021/CMPE-255/project/CMPE-255_S3-Group10/C50/C50test/'\n",
    "author_list=os.listdir(test_data_path)#lists all the authors in path 'train_data_path'\n",
    "test_labels=[]\n",
    "author_text_tuple=[]\n",
    "test_set=[]\n",
    "authindex=0\n",
    "#print('l',l)\n",
    "for p in author_list:\n",
    "    #print(p)\n",
    "    if (not p.startswith('.')):\n",
    "        file_list=os.listdir(test_data_path+p)#lists all the files in the path \"test_data_path+p\"\n",
    "        #file_list.remove('.ipynb_checkpoints') # remove .ipynb_checkpoints if data is present  in jupyter notebook\n",
    "        #file_list.remove('.DS_store') # remove .DS_store files if data is prsent in local drive\n",
    "        path2_each_author=test_data_path+p#path for every author folder\n",
    "        #print('file_list',file_list)\n",
    "        \n",
    "        for author in author_list:\n",
    "            if (not author.startswith('.')):  # this condition is to keep out all hidden files\n",
    "                if author not in test_labels:\n",
    "                    test_labels.append(author)\n",
    "        writings = ''\n",
    "        for file in range(len(file_list)):\n",
    "            #print(path2_each_author+'/'+file)\n",
    "            with open (path2_each_author+'/'+file_list[file],'r') as fp:\n",
    "                writings+=fp.read()\n",
    "                #author_text_tuple.append((author_list[authindex],file_list[file],(fp.read().splitlines())))\n",
    "        test_set.append(writings)\n",
    "        #authindex+=1\n",
    "print(\"Number of test labels:\",len(train_labels))\n",
    "print(\"Test labels\",train_labels)#list of authors which are class labels\n",
    "print(\"-\"*50)\n",
    "#print(author_text_tuple)# list of tuples with filename,content\n",
    "#print(test_set)# print combined text from each author \n",
    "\n",
    "# some data preprocessing todo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028d7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
